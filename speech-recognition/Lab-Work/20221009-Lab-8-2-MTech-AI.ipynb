{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMk1lnQg7tfsTA/eVVl+6S7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilalProgTech/mtech-nmims/blob/master/speech-recognition/Lab-Work/20221009-Lab-8-2-MTech-AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "110ru_HdLI4P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR']='/content'\n",
        "!kaggle datasets download -d sourabhy/hindi-speech-recognition\n",
        "!unzip hindi-speech-recognition.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "jHcaa_0XLPZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_num(filenum):\n",
        "    filenum = str(filenum)\n",
        "    if len(filenum) == 3:\n",
        "        filenum = '0' + filenum\n",
        "    return filenum"
      ],
      "metadata": {
        "id": "k8hGG6C3LPWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_trans = pd.read_csv('/content/test/transcription.txt', sep='_', header=None)\n",
        "actual_trans.columns = ['file', 'transcription']\n",
        "actual_trans.head()"
      ],
      "metadata": {
        "id": "XbepcSzCLPT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_trans['file'] = actual_trans['file'].apply(lambda x: preprocessing_num(x))\n",
        "actual_trans.head()"
      ],
      "metadata": {
        "id": "neSAzYlLLPRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_trans['file'] = '/content/test/audio/' + actual_trans['file'] + '_' + actual_trans['transcription'].str[:3] + '.wav'\n",
        "actual_trans.head()"
      ],
      "metadata": {
        "id": "IXxyOWogLPOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_trans['transcription'] = actual_trans['transcription'].str[3:]\n",
        "actual_trans.head()"
      ],
      "metadata": {
        "id": "_Dr7_QDcLPMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mfcc_features(path):\n",
        "    mfccs = []\n",
        "    try:\n",
        "        x , sr = librosa.load(path, res_type='kaiser_fast')\n",
        "        mfccs = librosa.feature.mfcc(y=x, sr=sr, n_mfcc=128)\n",
        "        mfccs = np.mean(mfccs.T,axis=0)\n",
        "    except:\n",
        "        print('Error reading audio')\n",
        "    return mfccs"
      ],
      "metadata": {
        "id": "Uq687kBkLPJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_trans.shape"
      ],
      "metadata": {
        "id": "t7VkRuMJLPGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfcc_features = {}\n",
        "for audio_path in actual_trans['file'].values:\n",
        "    path = audio_path.split('/')[-1]\n",
        "    mfcc_features[path] = create_mfcc_features(audio_path)"
      ],
      "metadata": {
        "id": "pRZPzTrCLdo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_trans['transcription'] = 'startseq ' + actual_trans['transcription'] + ' endseq'"
      ],
      "metadata": {
        "id": "-QJ4ROAyLdmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = actual_trans['transcription'].values"
      ],
      "metadata": {
        "id": "n7jY-sSPLdjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max(len(caption.split()) for caption in sentences)"
      ],
      "metadata": {
        "id": "QSTJky-lLdgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_input(df):\n",
        "    X_feature, X_seq, y = list(), list(), list()\n",
        "    audio_files = df['file'].tolist()\n",
        "\n",
        "    for audio_file in audio_files:\n",
        "        path = audio_file.split('/')[-1]\n",
        "        feature = mfcc_features[path].reshape(16, 8, 1)\n",
        "        captions = df.loc[df['file']==audio_file, 'transcription'].tolist()\n",
        "        for caption in captions:\n",
        "            sequence = tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "            for i in range(1, len(sequence)):\n",
        "                in_sequence, out_sequence = sequence[:i], sequence[i]\n",
        "                in_sequence = tf.keras.preprocessing.sequence.pad_sequences([in_sequence], maxlen=max_length)[0]\n",
        "                out_sequence = tf.keras.utils.to_categorical([out_sequence], num_classes=vocab_size)[0]\n",
        "                X_feature.append(feature)\n",
        "                X_seq.append(in_sequence)\n",
        "                y.append(out_sequence)\n",
        "\n",
        "    X_feature, X_seq, y = np.array(X_feature), np.array(X_seq), np.array(y)\n",
        "    return X_feature, X_seq, y"
      ],
      "metadata": {
        "id": "RDfAmcL6Lddq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(actual_trans, test_size=0.2)\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "JqnprlXxLPDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "id": "EjuECQ6lLPBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length"
      ],
      "metadata": {
        "id": "spywvFFzLr3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_feature, train_seq, train_y = get_model_input(train)\n",
        "test_feature, test_seq, test_y = get_model_input(test)\n",
        "\n",
        "train_feature.shape, train_seq.shape, train_y.shape"
      ],
      "metadata": {
        "id": "Cq5wWBVPLr1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout, Flatten, Dense, Input, Layer\n",
        "from keras.layers import Embedding, LSTM, add, Concatenate, Conv2D, MaxPooling2D\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "U3S7fKM-Lryx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "input_model_1 = Input(shape=(16, 8, 1))\n",
        "feature_model = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding = \"same\")(input_model_1)\n",
        "feature_model = MaxPooling2D(2, 2)(feature_model)\n",
        "feature_model = Conv2D(filters=8, kernel_size=(3, 3), activation='relu', padding = \"same\")(feature_model)\n",
        "feature_model = MaxPooling2D(2, 2)(feature_model)\n",
        "feature_model = Flatten()(feature_model)\n",
        "feature_model = Dense(256, activation='relu')(feature_model)\n",
        "\n",
        "# LSTM  model\n",
        "input_model_2 = Input(shape=(max_length,))\n",
        "seq_model = Embedding(vocab_size, 128, mask_zero=True)(input_model_2)\n",
        "seq_model = Dropout(0.5)(seq_model)\n",
        "seq_model = LSTM(256, activation='relu')(seq_model)\n",
        "\n",
        "# Merging both models\n",
        "decoder = add([feature_model, seq_model])\n",
        "decoder = Dense(256, activation='relu')(decoder)\n",
        "output_model = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "# [image, seq] [word]\n",
        "model = Model(inputs=[input_model_1, input_model_2], outputs=output_model)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "4Djy9Ul4LrwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model)"
      ],
      "metadata": {
        "id": "58vmp4w-Lrtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit((train_feature, train_seq), train_y, epochs=20,\n",
        "                    validation_data=((test_feature, test_seq), test_y))"
      ],
      "metadata": {
        "id": "fQnP9r_UL01U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_tokenizer(index):\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i==index:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "id": "rtq4BbSML0vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def speech_to_text(audio_path):\n",
        "    path = audio_path.split('/')[-1]\n",
        "    feature = mfcc_features[path].reshape(-1, 16, 8, 1)\n",
        "    pred_text = \"startseq\"\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([pred_text])[0]\n",
        "        sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], max_length)\n",
        "\n",
        "        y_pred = model.predict([feature,sequence])\n",
        "        y_pred = np.argmax(y_pred)\n",
        "        word = inverse_tokenizer(y_pred)\n",
        "        if word is None:\n",
        "            break\n",
        "        pred_text+= \" \" + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return pred_text"
      ],
      "metadata": {
        "id": "SsOYMxLDL0tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df = test.sample(12)\n",
        "files = sample_df['file'].tolist()\n",
        "actual_transcription = sample_df['transcription'].tolist()\n",
        "pred_transcription = []\n",
        "for file in files:\n",
        "    transcription = speech_to_text(file)\n",
        "    pred_transcription.append(transcription)"
      ],
      "metadata": {
        "id": "8Xod7dwUL6rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_transcription"
      ],
      "metadata": {
        "id": "OkFjAQbzL6pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df['transcription'].tolist()"
      ],
      "metadata": {
        "id": "4seI58bVL6mX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}